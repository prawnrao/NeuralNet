# Toy Neural Network Library
## With a custom matrix math library.
My inspiration for this is The Coding Train (Daniel Shiffman) and his [Toy Neural Network](https://github.com/CodingTrain/Toy-Neural-Network-JS) library in javascript. He has a fantastic [video series](https://www.youtube.com/watch?v=XJ7HLz9VYz0&list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh) on YouTube in which he goes into detail of exactly how a Neural Network functions.

I also referenced the 3Blue1Brown's (Grant Sanderson's) [video series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) on YouTube, which is where I understood most of the underlying mathematics of Neural Networks.

Tariq Rashid's book [Make Your Own Neural Network](https://www.amazon.com/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G/) is also an excellent source of in depth underlying mathematics with more than enough explanation to understand the derivations. He also has a [github repo](https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork) within which is the code he has referenced in his book.

I built this toy network for my own understanding of how of neural networks are able to solve problems from the ground up. This will build my foundation to explore further and attempt to solve more complex problems in the future, probably with a more optimised library, *cough cough* TensorFlow *cough cough*.

## Solved:
* Xor
* Added Numpy Compatibility
* Added mutiple hidden layer Compatibility

## Working on:
*  Test with [Theano](http://deeplearning.net/software/theano/)
*  Google [Quickdraw](https://quickdraw.withgoogle.com) classification

## Future Work:
*  MINST Handwritten numbers


## Further Reading
* Machine Learning for Artists [ML4A](https://ml4a.github.io/ml4a/how_neural_networks_are_trained/)
* Welch Labs Video Series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU)
